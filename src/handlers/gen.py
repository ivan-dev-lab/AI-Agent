# -*- coding: utf-8 -*-
from aiogram import Router, F
from aiogram.types import CallbackQuery, Message
from io import BytesIO

from config import ENABLE_GEN, DEFAULT_MODEL, PROMPT, PARSER, build_llm
from keyboards import back_kb
from callbacks import CB_GEN
from utils import extract_code_from_markdown, make_py_document

router = Router()

@router.callback_query(F.data == CB_GEN)
async def cb_gen(cq: CallbackQuery):
    from handlers.text import USER_STATE
    USER_STATE[cq.from_user.id] = {"mode": "gen", "step": 0, "data": {}, "chat_id": cq.message.chat.id}
    await cq.message.edit_text(
        "ü§ñ <b>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è MicroPython</b>\n\n"
        "–®–∞–≥ 1/1: –æ–ø–∏—à–∏—Ç–µ –∑–∞–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–º. –ü—Ä–∏–º–µ—Ä:\n"
        "<i>–ö–Ω–æ–ø–∫–∞ –Ω–∞ GPIO12 –≤–∫–ª—é—á–∞–µ—Ç LED –Ω–∞ GPIO2 –Ω–∞ 3 —Å–µ–∫—É–Ω–¥—ã, PWM 50%</i>\n",
        reply_markup=back_kb()
    )

async def _run_generation(msg: Message, desc: str):
    if not ENABLE_GEN:
        return await msg.answer("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–Ω–µ—Ç LangChain/Ollama).", reply_markup=back_kb())
    model_name = DEFAULT_MODEL
    llm = build_llm(model_name)
    chain = PROMPT | llm | PARSER
    await msg.answer("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –∫–æ–¥, –ø–æ–¥–æ–∂–¥–∏—Ç–µ 5‚Äì15 —Å–µ–∫—É–Ω–¥...", reply_markup=back_kb())
    try:
        result = await chain.ainvoke({"task_description": desc})
    except Exception as e:
        return await msg.answer(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏: {e}", reply_markup=back_kb())

    code = extract_code_from_markdown(result)
    file = make_py_document("micropython_task.py", code)
    await msg.answer_document(file, caption="–ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∏–Ω—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.", reply_markup=back_kb())
    if len(result) < 3500:
        await msg.answer(result, reply_markup=back_kb())
